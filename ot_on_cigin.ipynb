{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdmnWRoEaJcW/QqZMBUYCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshaSatyavardhan/ot-cigin/blob/main/ot_on_cigin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zwy8bWGT00xb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "407b173b-42cf-47b8-baca-4ef612e28f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CIGIN'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 98 (delta 46), reused 52 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (98/98), 4.15 MiB | 7.88 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/devalab/CIGIN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi\n",
        "!pip install dgl-cu100\n",
        "!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html\n",
        "!pip install mlflow\n",
        "!pip install torch==1.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
      ],
      "metadata": {
        "id": "HKZmyMgm_HQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bddd587a-0fab-4104-bbc1-9456d4b90ff8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rdkit-pypi) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from rdkit-pypi) (8.4.0)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.9.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement dgl-cu100 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for dgl-cu100\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu113\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu113-0.9.1.post1-cp39-cp39-manylinux1_x86_64.whl (239.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl-cu113) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl-cu113) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl-cu113) (5.9.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl-cu113) (3.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl-cu113) (1.10.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl-cu113) (2.27.1)\n",
            "Collecting numpydoc>=1.1.0\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dglgo) (0.7.0)\n",
            "Collecting ogb>=1.3.3\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from dglgo) (1.2.2)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.9/dist-packages (from dglgo) (2022.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.9/dist-packages (from dglgo) (6.0)\n",
            "Collecting autopep8>=1.6.0\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml>=0.17.20\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from dglgo) (1.10.6)\n",
            "Collecting isort>=5.10.1\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycodestyle>=2.10.0\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.9/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Collecting sphinx>=4.2\n",
            "  Downloading sphinx-6.1.3-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.9/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.26.15)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.4.4)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.13.1+cu116)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.9.0->dglgo) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl-cu113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl-cu113) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl-cu113) (2.0.12)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.4/519.4 KB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from rdkit-pypi->dglgo) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.2)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (63.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Collecting Pygments>=2.13\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: babel>=2.9 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (6.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.0)\n",
            "Collecting docutils<0.20,>=0.18\n",
            "  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 KB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8->sphinx>=4.2->numpydoc>=1.1.0->dglgo) (3.15.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=6649fcc02fde49ec5dc5e1eff9fe4a15fc577834062e2d215ffb8d527cb033f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, Pygments, pycodestyle, isort, docutils, sphinx, ruamel.yaml, outdated, dgl-cu113, autopep8, ogb, numpydoc, dglgo\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pygments-2.14.0 autopep8-2.0.2 dgl-cu113-0.9.1.post1 dglgo-0.0.2 docutils-0.19 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.5 outdated-0.2.2 pycodestyle-2.10.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 sphinx-6.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.2.2-py3-none-any.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2.27.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.10.1)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.19.6)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.4.1)\n",
            "Collecting alembic<2\n",
            "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (9.0.0)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.9/dist-packages (from mlflow) (23.0)\n",
            "Collecting gunicorn<21\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting databricks-cli<1,>=0.8.7\n",
            "  Downloading databricks-cli-0.17.5.tar.gz (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker<7,>=4.0.0\n",
            "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.4.46)\n",
            "Collecting gitpython<4,>=2.1.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.9/dist-packages (from mlflow) (0.4)\n",
            "Collecting shap<1,>=0.40\n",
            "  Downloading shap-0.41.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.4/572.4 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.22.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.9/dist-packages (from mlflow) (6.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2.2.1)\n",
            "Collecting querystring-parser<2\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2022.7.1)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (0.4.3)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.4.4)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic<2->mlflow) (4.5.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.9/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.10)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.15.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker<7,>=4.0.0->mlflow) (1.5.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.9/dist-packages (from docker<7,>=4.0.0->mlflow) (1.26.15)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from Flask<3->mlflow) (2.2.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from Flask<3->mlflow) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m367.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.9/dist-packages (from gunicorn<21->mlflow) (63.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (4.39.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (5.12.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.17.3->mlflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.17.3->mlflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2->mlflow) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2->mlflow) (1.1.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from shap<1,>=0.40->mlflow) (0.56.4)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.9/dist-packages (from shap<1,>=0.40->mlflow) (4.65.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->shap<1,>=0.40->mlflow) (0.39.1)\n",
            "Building wheels for collected packages: databricks-cli\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.5-py3-none-any.whl size=143016 sha256=d5e24c0da1d45a3118ebeb3463fc6a8866f2aaf0078a6a6bce1b42de44967012\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ac/b1/2c75e46c6ffb00ed09b3c94577a1ea387b75289a2ee04f247d\n",
            "Successfully built databricks-cli\n",
            "Installing collected packages: smmap, slicer, querystring-parser, pyjwt, Mako, gunicorn, gitdb, docker, databricks-cli, alembic, shap, gitpython, mlflow\n",
            "Successfully installed Mako-1.2.4 alembic-1.10.2 databricks-cli-0.17.5 docker-6.0.1 gitdb-4.0.10 gitpython-3.1.31 gunicorn-20.1.0 mlflow-2.2.2 pyjwt-2.6.0 querystring-parser-1.2.4 shap-0.41.0 slicer-0.0.7 smmap-5.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
            "Collecting torch==1.10.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.0%2Bcu113-cp39-cp39-linux_x86_64.whl (1821.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m905.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.0+cu113) (4.5.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.10.0+cu113 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.0+cu113 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.10.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.10.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P_y0FpWW1hX",
        "outputId": "bf541cdf-1388-4fc6-e45a-681ec03ba5a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/sample_data\n",
        "!mv /content/CIGIN/* ./"
      ],
      "metadata": {
        "id": "SL6jdC7tsjRA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from dgl import DGLGraph, heterograph\n",
        "from dgl.nn.pytorch import Set2Set, NNConv, GATConv\n",
        "from rdkit import Chem, RDLogger,rdBase\n",
        "from rdkit.Chem import rdMolDescriptors as rdDesc\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "# rdkit imports\n",
        "from rdkit import RDLogger\n",
        "from rdkit import rdBase\n",
        "from rdkit import Chem\n",
        "\n",
        "# torch imports\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch\n",
        "\n",
        "#dgl imports\n",
        "import dgl\n",
        "\n",
        "\n",
        "lg = RDLogger.logger()\n",
        "lg.setLevel(RDLogger.CRITICAL)\n",
        "rdBase.DisableLog('rdApp.error')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "xlo-WBlTOagc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41170c6a-fdd0-40ce-ca50-cbddcb3718bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatherModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 node_input_dim=42,\n",
        "                 edge_input_dim=10,\n",
        "                 node_hidden_dim=42,\n",
        "                 edge_hidden_dim=128,\n",
        "                 num_step_message_passing=6,\n",
        "                 gather=\"mpnn\",\n",
        "                 n_heads=3):\n",
        "        super(GatherModel, self).__init__()\n",
        "        self.num_step_message_passing = num_step_message_passing\n",
        "        self.lin0 = nn.Linear(node_input_dim, node_hidden_dim)\n",
        "        self.gather = gather\n",
        "        self.set2set = Set2Set(node_hidden_dim, 2, 1) \n",
        "        if self.gather == \"mpnn\":\n",
        "        \tself.message_layer = nn.Linear(2 * node_hidden_dim, node_hidden_dim)\n",
        "\t        edge_network = nn.Sequential(\n",
        "\t            nn.Linear(edge_input_dim, edge_hidden_dim), nn.ReLU(),\n",
        "\t            nn.Linear(edge_hidden_dim, node_hidden_dim * node_hidden_dim))\n",
        "\t        self.conv = NNConv(in_feats=node_hidden_dim,\n",
        "\t                           out_feats=node_hidden_dim,\n",
        "\t                           edge_func=edge_network,\n",
        "\t                           aggregator_type='sum',\n",
        "                               residual=True\n",
        "                                )\n",
        "        \tself.gru = nn.GRU(node_hidden_dim, node_hidden_dim)\n",
        "        elif self.gather == \"gat\":\n",
        "        \tself.n_heads = n_heads  \n",
        "        \tself.gat =  GATConv(node_hidden_dim,node_hidden_dim,self.n_heads)\n",
        "\n",
        "    def forward(self, g, n_feat, e_feat):\n",
        "\n",
        "        init = n_feat.clone()\n",
        "        out = F.relu(self.lin0(n_feat))\n",
        "        if self.gather == \"mpnn\":\n",
        "            h = out.unsqueeze(0)                           \n",
        "            for i in range(self.num_step_message_passing):\n",
        "                m = torch.relu(self.conv(g, out, e_feat))\n",
        "                out = self.message_layer(torch.cat([m, out],dim=1))\n",
        "            return out + init\n",
        "\n",
        "\n",
        "\n",
        "class CIGINModel(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 node_input_dim=42,\n",
        "                 edge_input_dim=10,\n",
        "                 node_hidden_dim=42,\n",
        "                 edge_hidden_dim=42,\n",
        "                 num_step_message_passing=8,\n",
        "                 interaction='dot',\n",
        "                 gather='mpnn'):\n",
        "        super(CIGINModel, self).__init__()\n",
        "        \n",
        "        self.node_input_dim = node_input_dim\n",
        "        self.node_hidden_dim =  node_hidden_dim\n",
        "        self.edge_input_dim = edge_input_dim\n",
        "        self.edge_hidden_dim = edge_hidden_dim\n",
        "        self.num_step_message_passing = num_step_message_passing\n",
        "        self.gather = gather\n",
        "        self.interaction = interaction\n",
        "\n",
        "        self.solute_gather = GatherModel(self.node_input_dim,self.edge_input_dim,\n",
        "                              self.node_hidden_dim,self.edge_input_dim,\n",
        "                              self.num_step_message_passing, \n",
        "                              self.gather, 3)\n",
        "        self.solvent_gather = GatherModel(self.node_input_dim,self.edge_input_dim,\n",
        "                              self.node_hidden_dim,self.edge_input_dim,\n",
        "                              self.num_step_message_passing, \n",
        "                              self.gather, 3)\n",
        "\n",
        "        self.fc1 = nn.Linear(8*self.node_hidden_dim,256)\n",
        "        self.fc2 = nn.Linear(256,128)\n",
        "        self.fc3 = nn.Linear(128,1)\n",
        "        \n",
        "        self.imap = nn.Linear(80,1)\n",
        "        self.num_step_set2set=2\n",
        "        self.num_layer_set2set=1\n",
        "        self.set2set_solute = Set2Set(2*node_hidden_dim, self.num_step_set2set, self.num_layer_set2set)\n",
        "        self.set2set_solvent = Set2Set(2*node_hidden_dim, self.num_step_set2set, self.num_layer_set2set)\n",
        "\n",
        "    # def solu_feature(self):\n",
        "    #   solute_H = torch.ones(self.solute_features.shape[0], self.solute_features.shape[1]) / self.solute_features.shape[1]\n",
        "    #   return solute_H\n",
        "\n",
        "    # def solv_features(self):\n",
        "    #   solvent_H = torch.ones(self.solvent_features.shape[0], self.solvent_features.shape[1]) / self.solvent_features.shape[1]\n",
        "    #   return solvent_H\n",
        "        \n",
        "    def forward(self, data):\n",
        "\n",
        "        solute = data[0]\n",
        "        solvent = data[1]\n",
        "\n",
        "        solute_features = self.solute_gather(solute, solute.ndata['x'].float(), solute.edata['w'].float())\n",
        "        solvent_features = self.solvent_gather(solvent, solvent.ndata['x'].float(), solvent.edata['w'].float())\n",
        "\n",
        "\n",
        "        if 'dot' not in self.interaction:\n",
        "            X1 = solute_features.unsqueeze(0)\n",
        "            Y1= solvent_features.unsqueeze(1)\n",
        "            X2 = X1.repeat(solvent_features.shape[0],1,1)\n",
        "            Y2 = Y1.repeat(1,solute_features.shape[0],1)\n",
        "            Z = torch.cat([X2,Y2],-1)\n",
        "\n",
        "            if self.interaction == 'general':\n",
        "                interaction_map = self.imap(Z).squeeze(2)\n",
        "            if self.interaction == 'tanh-general':\n",
        "                interaction_map = torch.tanh(self.imap(Z)).squeeze(2)\n",
        "\n",
        "            ret_interaction_map = torch.clone(interaction_map)\n",
        "\n",
        "        elif 'dot' in self.interaction :\n",
        "            interaction_map = torch.mm(solute_features, solvent_features.t())\n",
        "            if 'scaled' in self.interaction:\n",
        "                interaction_map = interaction_map/(np.sqrt(self.node_hidden_dim))\n",
        "\n",
        "            ret_interaction_map = torch.clone(interaction_map)\n",
        "            interaction_map = torch.tanh(interaction_map)\n",
        "        \n",
        "        solvent_prime = torch.mm(interaction_map.t(), solute_features)\n",
        "        solute_prime = torch.mm(interaction_map, solvent_features)\n",
        "\n",
        "        solute_features = torch.cat((solute_features, solute_prime), dim=1)\n",
        "        solvent_features = torch.cat((solvent_features, solvent_prime), dim=1)\n",
        "        \n",
        "        \n",
        "        solute_features = self.set2set_solute(solute, solute_features)\n",
        "        solvent_features = self.set2set_solvent(solvent, solvent_features)\n",
        "\n",
        "        final_features = torch.cat((solute_features,solvent_features),1)\n",
        "        predictions = torch.relu(self.fc1(final_features))\n",
        "        predictions = torch.relu(self.fc2(predictions))\n",
        "        predictions =  self.fc3(predictions)\n",
        "        return predictions, ret_interaction_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(\n",
        "            x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]        \n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def get_atom_features(atom, stereo, features, explicit_H=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Method that computes atom level features from rdkit atom object\n",
        "    :param atom: rdkit atom object\n",
        "    :return: atom features, 1d numpy array\n",
        "    \"\"\"\n",
        "    # todo: take list  of all possible atoms\n",
        "    possible_atoms = ['C','N','O','S','F','P','Cl','Br','I','Si']\n",
        "    atom_features  = one_of_k_encoding_unk(atom.GetSymbol(),possible_atoms)\n",
        "    atom_features += one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1])\n",
        "    atom_features += one_of_k_encoding_unk(atom.GetNumRadicalElectrons(), [0, 1])\n",
        "    atom_features += one_of_k_encoding(atom.GetDegree(),[0, 1, 2, 3, 4, 5, 6]) \n",
        "    atom_features += one_of_k_encoding_unk(atom.GetFormalCharge(), [-1, 0, 1])\n",
        "    atom_features += one_of_k_encoding_unk(atom.GetHybridization(), [\n",
        "                Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
        "                Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D])\n",
        "    atom_features += [int(i) for i in list(\"{0:06b}\".format(features))]\n",
        "\n",
        "    #todo: add aromacity,acceptor,donor and chirality\n",
        "    if not explicit_H:\n",
        "        atom_features += one_of_k_encoding_unk(atom.GetTotalNumHs(),[0, 1, 2, 3, 4])\n",
        "\n",
        "    try:\n",
        "        atom_features += one_of_k_encoding_unk(stereo,['R', 'S']) \n",
        "        atom_features += [atom.HasProp('_ChiralityPossible')]\n",
        "    except Exception as e:\n",
        "        \n",
        "        atom_features +=  [False, False\n",
        "                          ] + [atom.HasProp('_ChiralityPossible')]\n",
        "        \n",
        "    return np.array(atom_features)\n",
        "\n",
        "def get_bond_features(bond):\n",
        "    \n",
        "    \"\"\"\n",
        "    Method that computes bond level features from rdkit bond object\n",
        "    :param bond: rdkit bond object\n",
        "    :return: bond features, 1d numpy array\n",
        "    \"\"\"\n",
        "    \n",
        "    bond_type = bond.GetBondType()\n",
        "    bond_feats = [\n",
        "      bond_type == Chem.rdchem.BondType.SINGLE, bond_type == Chem.rdchem.BondType.DOUBLE,\n",
        "      bond_type == Chem.rdchem.BondType.TRIPLE, bond_type == Chem.rdchem.BondType.AROMATIC,\n",
        "      bond.GetIsConjugated(),\n",
        "      bond.IsInRing()\n",
        "    ]\n",
        "    bond_feats += one_of_k_encoding_unk(str(bond.GetStereo()),[\"STEREONONE\", \"STEREOANY\", \"STEREOZ\", \"STEREOE\"])\n",
        "\n",
        "    return np.array(bond_feats)\n",
        "\n",
        "def get_graph_from_smile(molecule):\n",
        "    \n",
        "    \"\"\"\n",
        "    Method that constructs a molecular graph with nodes being the atoms\n",
        "    and bonds being the edges.\n",
        "    :param molecule: SMILE sequence\n",
        "    :return: DGL graph object, Node features and Edge features\n",
        "    \"\"\"\n",
        "\n",
        "    G = DGLGraph()\n",
        "    molecule = Chem.MolFromSmiles(molecule)\n",
        "    features = rdDesc.GetFeatureInvariants(molecule)\n",
        "    \n",
        "    stereo = Chem.FindMolChiralCenters(molecule)\n",
        "    chiral_centers = [0]* molecule.GetNumAtoms()\n",
        "    for i in stereo:\n",
        "        chiral_centers[i[0]] = i[1]\n",
        "        \n",
        "    G.add_nodes(molecule.GetNumAtoms())\n",
        "    node_features = []\n",
        "    edge_features = []\n",
        "    for i in range(molecule.GetNumAtoms()):\n",
        "\n",
        "        atom_i = molecule.GetAtomWithIdx(i)\n",
        "        atom_i_features =  get_atom_features(atom_i,chiral_centers[i],features[i])\n",
        "        node_features.append(atom_i_features)\n",
        "        \n",
        "        for j in range(molecule.GetNumAtoms()):\n",
        "            bond_ij = molecule.GetBondBetweenAtoms(i, j)\n",
        "            if bond_ij is not None:\n",
        "                G.add_edge(i,j)\n",
        "                bond_features_ij = get_bond_features(bond_ij)\n",
        "                edge_features.append(bond_features_ij)\n",
        "                \n",
        "    G.ndata['x'] = torch.FloatTensor(node_features)\n",
        "    G.edata['w'] = torch.FloatTensor(edge_features)\n",
        "    return G\n",
        "\n",
        "\n",
        "\n",
        "def get_len_matrix(len_list):\n",
        "    len_list = np.array(len_list)\n",
        "    max_nodes = np.sum(len_list)\n",
        "    curr_sum = 0\n",
        "    len_matrix = []\n",
        "    for l in len_list:\n",
        "        curr = np.zeros(max_nodes)\n",
        "        curr[curr_sum:curr_sum+l] = 1\n",
        "        len_matrix.append(curr)\n",
        "        curr_sum += l\n",
        "    return np.array(len_matrix)\n",
        "    \n",
        "class Dataclass(Dataset):\n",
        "    def __init__(self,dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # solute_file = 'mol_files/'+self.dataset.loc[idx]['FileHandle'] +'.mol'\n",
        "        # solute = Chem.MolFromMolFile(solute_file) \n",
        "        # solute=Chem.MolToSmiles(solute)\n",
        "        solute = self.dataset.loc[idx]['SoluteSMILES']\n",
        "        mol = Chem.MolFromSmiles(solute)\n",
        "        mol = Chem.AddHs(mol)\n",
        "        solute = Chem.MolToSmiles(mol)\n",
        "        solute_graph = get_graph_from_smile(solute)\n",
        "        \n",
        "        solvent = self.dataset.loc[idx]['SolventSMILES']\n",
        "        mol = Chem.MolFromSmiles(solvent)\n",
        "        mol = Chem.AddHs(mol)\n",
        "        solvent = Chem.MolToSmiles(mol)\n",
        "        \n",
        "        solvent_graph = get_graph_from_smile(solvent)\n",
        "        ddi_value = self.dataset.loc[idx]['DeltaGsolv']\n",
        "        return [solute_graph, solvent_graph, ddi_value]\n",
        "\n",
        "model= CIGINModel().to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omwq1ydnOb9-",
        "outputId": "a6f9a560-acdf-4e63-af44-07059014fbc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CIGINModel(\n",
              "  (solute_gather): GatherModel(\n",
              "    (lin0): Linear(in_features=42, out_features=42, bias=True)\n",
              "    (set2set): Set2Set(\n",
              "      n_iters=2\n",
              "      (lstm): LSTM(84, 42)\n",
              "    )\n",
              "    (message_layer): Linear(in_features=84, out_features=42, bias=True)\n",
              "    (conv): NNConv(\n",
              "      (edge_func): Sequential(\n",
              "        (0): Linear(in_features=10, out_features=10, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=10, out_features=1764, bias=True)\n",
              "      )\n",
              "      (res_fc): Identity()\n",
              "    )\n",
              "    (gru): GRU(42, 42)\n",
              "  )\n",
              "  (solvent_gather): GatherModel(\n",
              "    (lin0): Linear(in_features=42, out_features=42, bias=True)\n",
              "    (set2set): Set2Set(\n",
              "      n_iters=2\n",
              "      (lstm): LSTM(84, 42)\n",
              "    )\n",
              "    (message_layer): Linear(in_features=84, out_features=42, bias=True)\n",
              "    (conv): NNConv(\n",
              "      (edge_func): Sequential(\n",
              "        (0): Linear(in_features=10, out_features=10, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=10, out_features=1764, bias=True)\n",
              "      )\n",
              "      (res_fc): Identity()\n",
              "    )\n",
              "    (gru): GRU(42, 42)\n",
              "  )\n",
              "  (fc1): Linear(in_features=336, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (imap): Linear(in_features=80, out_features=1, bias=True)\n",
              "  (set2set_solute): Set2Set(\n",
              "    n_iters=2\n",
              "    (lstm): LSTM(168, 84)\n",
              "  )\n",
              "  (set2set_solvent): Set2Set(\n",
              "    n_iters=2\n",
              "    (lstm): LSTM(168, 84)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(samples):\n",
        "    solute_graphs, solvent_graphs, labels = map(list, zip(*samples))\n",
        "    solute_graphs = dgl.batch(solute_graphs)\n",
        "    solvent_graphs = dgl.batch(solvent_graphs)\n",
        "    solute_len_matrix = get_len_matrix(solute_graphs.batch_num_nodes())\n",
        "    solvent_len_matrix = get_len_matrix(solvent_graphs.batch_num_nodes())\n",
        "    return solute_graphs, solvent_graphs, solute_len_matrix, solvent_len_matrix, labels"
      ],
      "metadata": {
        "id": "2Zkg8McGOeW2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_name = 'cigin'\n",
        "interaction = 'dot'\n",
        "max_epochs = 10\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "a6jUusFOOeyK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/CIGIN_V2/data/train.csv', sep=\";\")\n",
        "valid_df = pd.read_csv('/content/CIGIN_V2/data/valid.csv', sep=\";\")\n",
        "\n",
        "train_dataset = Dataclass(train_df)\n",
        "valid_dataset = Dataclass(valid_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, collate_fn=collate, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, collate_fn=collate, batch_size=128)"
      ],
      "metadata": {
        "id": "daMAaOD2OjCy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "En1y35X_XsAD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CIGINModel(interaction=interaction)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, mode='min', verbose=True)"
      ],
      "metadata": {
        "id": "odUkFyaJOk_D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "mae_loss_fn = torch.nn.L1Loss()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "def get_metrics(model, data_loader):\n",
        "    valid_outputs = []\n",
        "    valid_labels = []\n",
        "    valid_loss = []\n",
        "    valid_mae_loss = []\n",
        "    for solute_graphs, solvent_graphs, solute_lens, solvent_lens, labels in tqdm(data_loader):\n",
        "        outputs, i_map = model(\n",
        "            [solute_graphs.to(device), solvent_graphs.to(device), torch.tensor(solute_lens).to(device),\n",
        "             torch.tensor(solvent_lens).to(device)])\n",
        "        loss = loss_fn(outputs, torch.tensor(labels).to(device).float())\n",
        "        mae_loss = mae_loss_fn(outputs, torch.tensor(labels).to(device).float())\n",
        "        valid_outputs += outputs.cpu().detach().numpy().tolist()\n",
        "        valid_loss.append(loss.cpu().detach().numpy())\n",
        "        valid_mae_loss.append(mae_loss.cpu().detach().numpy())\n",
        "        valid_labels += labels\n",
        "\n",
        "    loss = np.mean(np.array(valid_loss).flatten())\n",
        "    mae_loss = np.mean(np.array(valid_mae_loss).flatten())\n",
        "    return loss, mae_loss\n",
        "\n",
        "\n",
        "def train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name):\n",
        "    best_val_loss = 100\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        running_loss = []\n",
        "        tq_loader = tqdm(train_loader)\n",
        "        o = {}\n",
        "        for samples in tq_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs, interaction_map = model(\n",
        "                [samples[0].to(device), samples[1].to(device), torch.tensor(samples[2]).to(device),\n",
        "                 torch.tensor(samples[3]).to(device)])\n",
        "            l1_norm = torch.norm(interaction_map, p=2) * 1e-4\n",
        "            loss = loss_fn(outputs, torch.tensor(samples[4]).to(device).float()) + l1_norm\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss = loss - l1_norm\n",
        "            running_loss.append(loss.cpu().detach())\n",
        "            tq_loader.set_description(\n",
        "                \"Epoch: \" + str(epoch + 1) + \"  Training loss: \" + str(np.mean(np.array(running_loss))))\n",
        "        model.eval()\n",
        "        val_loss, mae_loss = get_metrics(model, valid_loader)\n",
        "        scheduler.step(val_loss)\n",
        "        print(\"Epoch: \" + str(epoch + 1) + \"  train_loss \" + str(np.mean(np.array(running_loss))) + \" Val_loss \" + str(\n",
        "            val_loss) + \" MAE Val_loss \" + str(mae_loss))\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"/content/runs/best_model.tar\")"
      ],
      "metadata": {
        "id": "qqF3NZsqOmnd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7v08-84OoA3",
        "outputId": "13aec50e-0703-42ee-8e8b-e657a7cd9da1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1  Training loss: 1.6648848: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1  train_loss 1.6648848 Val_loss 1.2317414 MAE Val_loss 0.95316017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2  Training loss: 1.2317415: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2  train_loss 1.2317415 Val_loss 1.6051244 MAE Val_loss 1.1040303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3  Training loss: 1.6051244: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3  train_loss 1.6051244 Val_loss 1.4120902 MAE Val_loss 1.0456204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4  Training loss: 1.4120902: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 13.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4  train_loss 1.4120902 Val_loss 1.1998557 MAE Val_loss 0.9066779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5  Training loss: 1.1998554: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5  train_loss 1.1998554 Val_loss 1.3612971 MAE Val_loss 1.0300183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 6  Training loss: 1.3612971: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6  train_loss 1.3612971 Val_loss 1.3032413 MAE Val_loss 1.0039868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 7  Training loss: 1.3032413: 100%|██████████| 1/1 [00:00<00:00,  8.03it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7  train_loss 1.3032413 Val_loss 1.2118188 MAE Val_loss 0.9358868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 8  Training loss: 1.2118188: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8  train_loss 1.2118188 Val_loss 1.2039841 MAE Val_loss 0.9187584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 9  Training loss: 1.203984: 100%|██████████| 1/1 [00:00<00:00,  7.35it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9  train_loss 1.203984 Val_loss 1.2364683 MAE Val_loss 0.9572525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 10  Training loss: 1.2364682: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    11: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch: 10  train_loss 1.2364682 Val_loss 1.2452238 MAE Val_loss 0.96423787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsXqrg2zVf0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}